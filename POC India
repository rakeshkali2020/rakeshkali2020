from pyspark.sql.functions import concat, col, lit from pyspark.sql import functions as F from datetime import datetime

Query to fetch required data including customer_id

pivotdf = spark.sql(""" SELECT dim.brand_desc, dim.PG1, dim.PG2, dim.PG3, dim.PG4, cust.customer_id, substring(month,3,2) AS year, CASE WHEN substring(month,5,2)='01' THEN 'JANUARY' WHEN substring(month,5,2)='02' THEN 'FEBRUARY' WHEN substring(month,5,2)='03' THEN 'MARCH' WHEN substring(month,5,2)='04' THEN 'APRIL' WHEN substring(month,5,2)='05' THEN 'MAY' WHEN substring(month,5,2)='06' THEN 'JUNE' WHEN substring(month,5,2)='07' THEN 'JULY' WHEN substring(month,5,2)='08' THEN 'AUGUST' WHEN substring(month,5,2)='09' THEN 'SEPTEMBER' WHEN substring(month,5,2)='10' THEN 'OCTOBER' WHEN substring(month,5,2)='11' THEN 'NOVEMBER' WHEN substring(month,5,2)='12' THEN 'DECEMBER' END AS calender_month,
tof_quantity, PDP_comp_quantity_cases, PDP_comp_value_usd FROM edap_transform.fa_monthly_snapshot_fct fct INNER JOIN edap_sa.sa_product_dim dim ON fct.global_item_number = dim.product INNER JOIN edap_sa.sa_customer_hierarchy cust ON fct.customer_key = cust.customer_key WHERE fct.month >= '202301' AND fct.month <= '202302' """)

Transform DataFrame

pivotdf = pivotdf.select( "customer_id", "brand_desc", "PG1", "PG2", "PG3", "PG4", "year", "calender_month", "tof_quantity", "PDP_comp_quantity_cases", "PDP_comp_value_usd", concat(col("calender_month"), lit("'"), col("year")).alias("month") ).drop("year", "calender_month")

Aggregate Data

agg_df = pivotdf.groupBy("customer_id", "brand_desc", "PG1", "PG2", "PG3", "PG4", "month").agg( F.sum("tof_quantity").cast("DECIMAL(38,3)").alias("tof_quantity"), F.sum("PDP_comp_quantity_cases").cast("DECIMAL(38,3)").alias("PDP_comp_quantity_cases"), F.sum("PDP_comp_value_usd").alias("PDP_comp_value_usd") )

Reshape Data

result_df = agg_df.selectExpr( "customer_id", "brand_desc", "month", "PG1", "PG2", "PG3", "PG4", "stack(3, 'tof_quantity', tof_quantity, 'PDP_comp_quantity_cases', PDP_comp_quantity_cases, 'PDP_comp_value_usd', PDP_comp_value_usd) as (KEY FIGURES, Total)" ).where("Total is not null")

df = result_df.groupBy("customer_id", "brand_desc", "PG1", "PG2", "PG3", "PG4", "KEY FIGURES").pivot("month").agg(F.sum("Total")).orderBy("customer_id", "brand_desc", "PG1", "PG2", "PG3", "PG4", "KEY FIGURES")

df.createOrReplaceTempView('ibp_view')

Generate dynamic column list

cols_list = df.columns r = [] final_str = "customer_id, brand_desc, PG1, PG2, PG3, PG4, KEY FIGURES" for row in cols_list: if row not in ("customer_id", "brand_desc", "PG1", "PG2", "PG3", "PG4", "KEY FIGURES"): row = row.replace("'", "-") + "-01" date_value = datetime.strptime(row, '%B-%y-%d').strftime('%B'%y') r.append(date_value) r.sort(key=lambda x: datetime.strptime(x, '%B'%y')) final_str += ", " + ", ".join(f'{i.upper()}' for i in r)

Execute final query

df_final = spark.sql(f"SELECT {final_str} FROM ibp_view")

